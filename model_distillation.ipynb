{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Distillation: Compressing Large Models into Smaller, Efficient Ones\n",
    "Model distillation (also called knowledge distillation) is a technique used in deep learning to compress a large, complex model (teacher) into a smaller, faster model (student) while retaining most of its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Why Use Model Distillation?\n",
    "üîπ Reduces computation & memory usage for deployment on edge devices (e.g., mobile, IoT, self-driving cars).\n",
    "üîπ Speeds up inference while maintaining high accuracy.\n",
    "üîπ Allows transfer of knowledge from large models (GPT, BERT, ResNet) to smaller ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". How Model Distillation Works\n",
    "Train a large, high-performance model (Teacher).\n",
    "Train a smaller model (Student) using:\n",
    "Soft Labels: The teacher provides probability distributions over classes.\n",
    "Hard Labels: Standard ground truth labels.\n",
    "Optimize the student model by minimizing the difference between:\n",
    "The student's predictions.\n",
    "The teacher's predictions (knowledge transfer).\n",
    "Distillation Loss Function\n",
    "\n",
    "L=(1‚àíŒ±)‚ãÖL \n",
    "hard\n",
    "‚Äã\n",
    " +Œ±‚ãÖT \n",
    "2\n",
    " ‚ãÖL \n",
    "soft\n",
    "‚Äã\n",
    " \n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "ùêø\n",
    "hard\n",
    "L \n",
    "hard\n",
    "‚Äã\n",
    "  = Cross-Entropy loss on ground truth labels.\n",
    "ùêø\n",
    "soft\n",
    "L \n",
    "soft\n",
    "‚Äã\n",
    "  = KL-Divergence loss on soft labels from the teacher.\n",
    "ùõº\n",
    "Œ± = Trade-off parameter.\n",
    "ùëá\n",
    "T = Temperature (smooths probabilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": Define Teacher and Student Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Load Pre-trained Teacher Model (ResNet-50)\n",
    "teacher_model = models.resnet50(pretrained=True)\n",
    "teacher_model.fc = nn.Linear(2048, 10)  # Modify for 10-class output\n",
    "teacher_model.eval()\n",
    "\n",
    "# Define Student Model (ResNet-18)\n",
    "student_model = models.resnet18(pretrained=False)\n",
    "student_model.fc = nn.Linear(512, 10)  # Same output as teacher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Distillation Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, temperature=3, alpha=0.5):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')  # Soft labels\n",
    "        self.ce_loss = nn.CrossEntropyLoss()  # Hard labels\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        soft_loss = self.kl_loss(\n",
    "            nn.functional.log_softmax(student_logits / self.temperature, dim=1),\n",
    "            nn.functional.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        )\n",
    "        hard_loss = self.ce_loss(student_logits, labels)\n",
    "        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Train the Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True),\n",
    "    batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "loss_fn = DistillationLoss(temperature=3, alpha=0.5)\n",
    "\n",
    "# Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward Pass (Teacher & Student)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(images)\n",
    "        student_logits = student_model(images)\n",
    "\n",
    "        # Compute Distillation Loss\n",
    "        loss = loss_fn(student_logits, teacher_logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variants of Model Distillation\n",
    "\n",
    "Type\tDescription\n",
    "\n",
    "Logit Matching\tStudent mimics teacher‚Äôs softmax output.\n",
    "\n",
    "Feature Distillation\tStudent learns intermediate layer representations.\n",
    "\n",
    "Self-Distillation\tA single model teaches itself (used in BERT, ViTs).\n",
    "\n",
    "Data-Free Distillation\tGenerates synthetic data to train the student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
